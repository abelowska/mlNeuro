{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMry/uwHS2E6MwJxcg6F0aC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abelowska/mlNeuro/blob/main/2025/MLN_p3_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple P300 Speller\n",
        "\n",
        "Detect when participant see target, and when non-target stimuli using [BI2015a dataset](https://neurotechx.github.io/moabb/generated/moabb.datasets.BI2015a.html#moabb.datasets.BI2015a)."
      ],
      "metadata": {
        "id": "ODKcAdRpEeOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moabb\n",
        "!pip install mne"
      ],
      "metadata": {
        "id": "XrOGdAUfEdad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, **restart your session** and then run next cells."
      ],
      "metadata": {
        "id": "JPxOyG6triQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "vYXtlf47OZum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import moabb\n",
        "import mne\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from mne.decoding import LinearModel, Vectorizer, get_coef"
      ],
      "metadata": {
        "id": "Y6GhZk2DEwvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers"
      ],
      "metadata": {
        "id": "zlG1Ar5-luWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_data(session='1'):\n",
        "  subject = 2\n",
        "  session = session\n",
        "  run = '0'\n",
        "\n",
        "  test_raw = data[subject][session][run]\n",
        "  # 1. re-reference: to almost-mastoids\n",
        "  test_raw.set_eeg_reference(ref_channels=['T7', 'T8'])\n",
        "\n",
        "  # 2. band-pass filter\n",
        "  test_raw_filtered = test_raw.copy().filter(\n",
        "      picks=['eeg'],\n",
        "      l_freq=.1,\n",
        "      h_freq=30.0,\n",
        "      n_jobs=10,\n",
        "      method='iir',\n",
        "      iir_params=None\n",
        "      )\n",
        "\n",
        "  # 3. Notch filter\n",
        "  power_freq = 50\n",
        "  nyquist_freq = test_raw_filtered.info['sfreq'] / 2\n",
        "\n",
        "  test_raw_filtered = test_raw_filtered.notch_filter(\n",
        "      picks=['eeg', 'eog'],\n",
        "      freqs=np.arange(power_freq, nyquist_freq, power_freq),\n",
        "      n_jobs=10,\n",
        "  )\n",
        "\n",
        "  # fing events on the STIM channel\n",
        "  events = mne.find_events(test_raw_filtered)\n",
        "\n",
        "  # create events dict\n",
        "  event_ids = {'Target': 2, 'Non-Target': 1}\n",
        "\n",
        "  # create segments\n",
        "  tmin = -0.2\n",
        "  tmax = 0.6\n",
        "  baseline = (-0.2,0)\n",
        "  test_epochs = mne.Epochs(\n",
        "      test_raw_filtered,\n",
        "      events,\n",
        "      event_id=event_ids,\n",
        "      tmin=tmin,\n",
        "      tmax=tmax,\n",
        "      baseline=baseline,\n",
        "      preload=True,\n",
        "  )\n",
        "\n",
        "  return test_epochs"
      ],
      "metadata": {
        "id": "5YDqzfCNlud7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data"
      ],
      "metadata": {
        "id": "2Y-wkDlIR_xP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Fetch data"
      ],
      "metadata": {
        "id": "G0UVYnuAOc53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhmbjyhwEbWO"
      },
      "outputs": [],
      "source": [
        "# Get data fro one subject. It might take a while\n",
        "dataset = moabb.datasets.BI2015a()\n",
        "data = dataset.get_data(subjects=[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "S1brgoQYSKIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract `MNE` `Raw` from the downloaded data"
      ],
      "metadata": {
        "id": "bm7SpMaAQTL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subject = 2\n",
        "session = '0'\n",
        "run = '0'\n",
        "\n",
        "raw = data[subject][session][run]\n",
        "raw"
      ],
      "metadata": {
        "id": "I5ZSua96JWVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Simple Raw pre-processing"
      ],
      "metadata": {
        "id": "cLiNdL9bQrWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = raw.plot()\n",
        "fig = raw.compute_psd().plot()\n",
        "# 1. re-reference: to almost-mastoids\n",
        "raw.set_eeg_reference(ref_channels=['T7', 'T8'])\n",
        "\n",
        "# 2. band-pass filter\n",
        "raw_filtered = raw.copy().filter(\n",
        "    picks=['eeg'],\n",
        "    l_freq=.1,\n",
        "    h_freq=30.0,\n",
        "    n_jobs=10,\n",
        "    method='iir',\n",
        "    iir_params=None\n",
        "    )\n",
        "\n",
        "# 3. Notch filter\n",
        "power_freq = 50\n",
        "nyquist_freq = raw_filtered.info['sfreq'] / 2\n",
        "\n",
        "raw_filtered = raw_filtered.notch_filter(\n",
        "    picks=['eeg', 'eog'],\n",
        "    freqs=np.arange(power_freq, nyquist_freq, power_freq),\n",
        "    n_jobs=10,\n",
        ")\n",
        "\n",
        "fig = raw_filtered.plot()\n",
        "fig = raw_filtered.compute_psd().plot()"
      ],
      "metadata": {
        "id": "1r11HXbAJ4Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create segments around stimuli"
      ],
      "metadata": {
        "id": "rlzcoWiIRDf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fing events on the STIM channel\n",
        "events = mne.find_events(raw_filtered)\n",
        "\n",
        "# create events dict\n",
        "event_ids = {'Target': 2, 'Non-Target': 1}\n",
        "\n",
        "# create segments\n",
        "tmin = -0.2\n",
        "tmax = 0.6\n",
        "baseline = (-0.2,0)\n",
        "epochs = mne.Epochs(\n",
        "    raw_filtered,\n",
        "    events,\n",
        "    event_id=event_ids,\n",
        "    picks=\"eeg\",\n",
        "    tmin=tmin,\n",
        "    tmax=tmax,\n",
        "    baseline=baseline,\n",
        "    preload=True\n",
        ")\n",
        "\n",
        "epochs"
      ],
      "metadata": {
        "id": "rts1LLkPJ6ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Look into EEG signal for target and non-target stimuli"
      ],
      "metadata": {
        "id": "BEeZPVJMRknz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create ERPs\n",
        "target_erp = epochs['Target'].average()\n",
        "nontarget_erp = epochs['Non-Target'].average()\n",
        "\n",
        "# compare target and non-target ERPs\n",
        "picks = ['Cz']\n",
        "\n",
        "fig = mne.viz.plot_compare_evokeds(\n",
        "    evokeds = {'target': target_erp, 'non-target': nontarget_erp},\n",
        "    picks=picks,\n",
        "    invert_y=True\n",
        ")"
      ],
      "metadata": {
        "id": "C20pRHVyLMXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA model on balanced data"
      ],
      "metadata": {
        "id": "2fIP4H5IR6PD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "8V7JF-_clDhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def balance_epochs_data(epochs):\n",
        "  y = epochs.events[:, -1]\n",
        "\n",
        "  # find class indices\n",
        "  classes, counts = np.unique(y, return_counts=True)\n",
        "  minority_class = classes[np.argmin(counts)]\n",
        "  majority_class = classes[np.argmax(counts)]\n",
        "\n",
        "  minority_indices = np.where(y == minority_class)[0]\n",
        "  majority_indices = np.where(y == majority_class)[0]\n",
        "\n",
        "  print(f\"Majority class ({majority_class}) n_samples: {len(majority_indices)}\")\n",
        "  print(f\"Minority class ({minority_class}) n_samples: {len(minority_indices)}\")\n",
        "\n",
        "  # Randomly choose majority samples to drop\n",
        "  np.random.seed(42)\n",
        "  majority_indices_to_drop = np.setdiff1d(\n",
        "      majority_indices,\n",
        "      np.random.choice(majority_indices, size=len(minority_indices), replace=False)\n",
        "  )\n",
        "\n",
        "  print(f\"Dropping: {len(majority_indices_to_drop)} samples from {majority_class} class.\")\n",
        "\n",
        "  # Drop the selected epochs\n",
        "  epochs_balanced = epochs.copy().drop(majority_indices_to_drop)\n",
        "  return epochs_balanced"
      ],
      "metadata": {
        "id": "4z0v7y2PlYbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_train_balanced = balance_epochs_data(epochs)"
      ],
      "metadata": {
        "id": "WzgjY634imEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA with MNE LinearModel"
      ],
      "metadata": {
        "id": "LduT3wQK0bNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### Creates data for fitting #################################\n",
        "tmin = 0.25\n",
        "tmax = 0.45\n",
        "picks = ['eeg']\n",
        "\n",
        "epochs_train_copy = epochs_train_balanced.copy()\n",
        "\n",
        "X_train = epochs_train_copy.pick(picks=picks).crop(tmin=tmin, tmax=tmax)\n",
        "y_train = epochs_train_copy.events[:, -1]\n",
        "\n",
        "###### Fit the simplest classification model #####################\n",
        "### But now use Pipelines, Standard Scaler and MNE LinearModel ###\n",
        "clf = make_pipeline(\n",
        "    Vectorizer(),  # vectorize across time and channels\n",
        "    StandardScaler(),  # normalize features across trials\n",
        "    LinearModel(  # fits a LDA regression\n",
        "        LinearDiscriminantAnalysis(solver='lsqr')\n",
        "    )\n",
        ").fit(X_train, y_train)\n",
        "y_predicted = clf.predict(X_train)\n",
        "\n",
        "\n",
        "###### Print classification results #################################\n",
        "print(f\"\\nTrain results:\\n{classification_report(y_true=y_train, y_pred=y_predicted)}\")\n",
        "\n",
        "#####################################################################\n",
        "###### Test the model on data from another session ##################\n",
        "#####################################################################\n",
        "epochs_test = get_test_data()\n",
        "epochs_test_balanced = balance_epochs_data(epochs_test)\n",
        "\n",
        "epochs_test_copy = epochs_test_balanced.copy()\n",
        "X_test = epochs_test_copy.pick(picks=picks).crop(tmin=tmin, tmax=tmax)\n",
        "y_test = epochs_test_copy.events[:, -1]\n",
        "\n",
        "y_test_predicted = clf.predict(X_test)\n",
        "print(f\"\\nTest results:\\n{classification_report(y_true=y_test, y_pred=y_test_predicted)}\")"
      ],
      "metadata": {
        "id": "frxLs0EZ0bUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot spatial patterns and filters"
      ],
      "metadata": {
        "id": "A3rTOrsT10xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and plot patterns and filters\n",
        "for name in (\"patterns_\", \"filters_\"):\n",
        "    # The `inverse_transform` parameter will call this method on any estimator\n",
        "    # contained in the pipeline, in reverse order.\n",
        "    coef = get_coef(clf, name, inverse_transform=True)\n",
        "    evoked = mne.EvokedArray(coef, X_test.info, tmin=X_test.tmin)\n",
        "    print(f\"EEG {name[:-1]}\")\n",
        "    fig = evoked.plot_topomap()"
      ],
      "metadata": {
        "id": "d7cyInWn1u1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play with models, regularizations, and patterns"
      ],
      "metadata": {
        "id": "Ivxzhf1OpVcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to impelment the following models:\n",
        "1. LDA with regularization (`shrinkage` parameter in `LinearDiscriminantAnalysis` class)\n",
        "2. Logistic Regression with L2\n",
        "3. LogisticRegression with L1\n",
        "\n",
        "\n",
        "Note, that for Logistic Regression, the parameter that controls the strength of the penalty (`C`) is $1 / \\lambda$"
      ],
      "metadata": {
        "id": "wBcf8GLRprWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "AskbpMq2qkZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}